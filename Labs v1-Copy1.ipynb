{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3462, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentimen</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>oks kak semangat ya kalian kalian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>sekarang harus kaya orang bodoh lagi bodoh sangat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Begitu diumumkan lulus 100%, mereka semua suju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[USERNAME] [USERNAME] Katanya Bapak Reformasi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>macet macetan perut kosong akhirnya mampir dah...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  sentimen                                              tweet\n",
       "0   1         1                  oks kak semangat ya kalian kalian\n",
       "1   2         0  sekarang harus kaya orang bodoh lagi bodoh sangat\n",
       "2   3         1  Begitu diumumkan lulus 100%, mereka semua suju...\n",
       "3   4         0  [USERNAME] [USERNAME] Katanya Bapak Reformasi ...\n",
       "4   5         0  macet macetan perut kosong akhirnya mampir dah..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Membaca data menggunakan pandas\n",
    "#import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import Sastrawi\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.tag import CRFTagger\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "raw_data = pd.read_csv('dataset/train_set.csv', encoding = \"Latin-1\")\n",
    "print(raw_data.shape)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Data Type Before Transformation \n",
      " id           int64\n",
      "sentimen     int64\n",
      "tweet       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (\"Column Data Type Before Transformation \\n\", raw_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Normalisasi\n",
    "\n",
    "def normalisasi(tweet):\n",
    "    normal_tw = tweet.lower() #lowercase\n",
    "    normal_tw = re.sub('\\s+', ' ', normal_tw) # remove extra space\n",
    "    normal_tw = normal_tw.strip() #trim depan belakang\n",
    "    normal_tw = re.sub(r'[^\\w\\s]','',normal_tw) #buang punctuation\n",
    "    normal_regex = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE) #regex huruf yang berulang kaya haiiii (untuk fitur unigram)\n",
    "    normal_tw = normal_regex.sub(r\"\\1\\1\", normal_tw) #buang huruf yang berulang\n",
    "    return normal_tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Remove Stopwords\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    stopwords = pd.read_csv(\"dataset/stopwords.csv\")\n",
    "    special_list = ['username', 'url', 'sensitive-no']\n",
    "    token = nltk.word_tokenize(tweet)\n",
    "    token_afterremoval = []\n",
    "    for k in token:\n",
    "        if k not in stopwords and k not in special_list:\n",
    "            token_afterremoval.append(k)\n",
    "    \n",
    "    str_clean = ' '.join(token_afterremoval)\n",
    "    return str_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Stemming\n",
    "\n",
    "def Stemming(tweet):\n",
    "    token = nltk.word_tokenize(tweet)\n",
    "    stem_kalimat = []\n",
    "    for k in token:\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "        stem_kata = stemmer.stem(clean_tw)\n",
    "        stem_kalimat.append(stem_kata)\n",
    "        \n",
    "    stem_kalimat_str = ' '.join(stem_kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oks kak semangat ya kalian kalian',\n",
       " 'sekarang harus kaya orang bodoh lagi bodoh sangat',\n",
       " 'begitu diumumkan lulus 100 mereka semua sujud syukur dan langsung mengambil bungasaat dia menghampiri langsung memeluk menciumku air mata tak kuasa kubendungmom this is my birthday present for u']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prapemrosesan Data\n",
    "\n",
    "def pra_pemrosesan(list_tweet):\n",
    "    tweet_clean = []\n",
    "    for tw in list_tweet:\n",
    "        normal_tweet = normalisasi(tw)\n",
    "        nosw_tweet = remove_stopwords(normal_tweet)\n",
    "        #stem_tweet = Stemming(nosw_tweet)\n",
    "        tweet_clean.append(nosw_tweet)\n",
    "    return tweet_clean\n",
    "\n",
    "raw_tweet = raw_data['tweet']\n",
    "label = raw_data['sentimen'].tolist()\n",
    "\n",
    "clean_tweet = pra_pemrosesan(raw_tweet)\n",
    "clean_tweet[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "['00', '0015', '011060039617518', '013150189591518', '02', '0217280858728086', '029', '06', '06062018', '0630']\n"
     ]
    }
   ],
   "source": [
    "# Ekstraksi Bag Of Word\n",
    "\n",
    "def EkstraksiBoW(tweet):\n",
    "    unigram = CountVectorizer(ngram_range=(1,1))\n",
    "    unigram_matrix = unigram.fit_transform(np.array(tweet)).todense()\n",
    "    nama_fitur = unigram.get_feature_names()\n",
    "    return unigram_matrix, nama_fitur, unigram\n",
    "\n",
    "unigram_feat, feat_name, unigram_used = EkstraksiBoW(clean_tweet)\n",
    "print(unigram_feat[:3])\n",
    "print(feat_name[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0], [0, 3], [1, 0], [2, 0], [0, 2], [0, 2], [0, 0], [2, 0], [1, 0], [0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Leksikon\n",
    "\n",
    "def EkstraksiSentimen(list_tweet):\n",
    "    pos = pd.read_csv(\"dataset/positif_vania.txt\", header=None, names=['pos'])\n",
    "    list_pos = pos['pos'].tolist()\n",
    "    neg = pd.read_csv(\"dataset/negatif_vania.txt\", header=None, names=['neg'])\n",
    "    list_neg = neg['neg'].tolist()\n",
    "    \n",
    "    fitur_sentimen_all = []\n",
    "    for tweet in list_tweet:\n",
    "        ##inisiasi value\n",
    "        emosi = [\"positif\", \"negatif\"]\n",
    "        value = [0,0]\n",
    "        emosi_value = {}\n",
    "        for i in range(len(emosi)):\n",
    "            emosi_value[emosi[i]] = value[i]\n",
    "        list_kata = tweet.split()\n",
    "        for k in list_kata:\n",
    "            if k in list_pos:\n",
    "                emosi_value[\"positif\"] += 1\n",
    "            if k in list_neg:\n",
    "                emosi_value[\"negatif\"] += 1\n",
    "        \n",
    "        \n",
    "        fitur_sentimen_perkalimat = list(emosi_value.values())\n",
    "        fitur_sentimen_all.append(fitur_sentimen_perkalimat)\n",
    "        \n",
    "    return fitur_sentimen_all\n",
    "\n",
    "sentlex_feat = EkstraksiSentimen(clean_tweet)\n",
    "print(sentlex_feat[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 0), (0, 1)]\n"
     ]
    }
   ],
   "source": [
    "# POSTag\n",
    "\n",
    "def EkstraksiPOS(list_tweet):\n",
    "    ct = CRFTagger()\n",
    "    ct.set_model_file(\"dataset/all_indo_man_tag_corpus_model.crf.tagger\")\n",
    "    pos_feat_list = []\n",
    "    count_tag = []\n",
    "    for tweet in list_tweet: \n",
    "        token = nltk.word_tokenize(tweet)\n",
    "        tag = ct.tag_sents([token])\n",
    "        flat_tag = [item for sublist in tag for item in sublist]\n",
    "        pos_count = Counter([j for i,j in flat_tag])\n",
    "        pos_feat = (pos_count['JJ'], pos_count['NEG'])\n",
    "        pos_feat_list.append(pos_feat)\n",
    "    return pos_feat_list\n",
    "\n",
    "postag_feat = EkstraksiPOS(clean_tweet)\n",
    "print(postag_feat[:3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 6, 33], [0, 0, 8, 49], [4, 0, 46, 219]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ortografi\n",
    "\n",
    "def EkstraksiOrtografi(raw_tweet):\n",
    "    all_orto_feat = []\n",
    "    for tw in raw_tweet:\n",
    "        capital_count = sum(1 for c in tw if c.isupper())\n",
    "        exclamation_count = sum((1 for c in tw if c == \"!\"))\n",
    "        word_len = len(nltk.word_tokenize(tw))\n",
    "        char_len = len(tw)\n",
    "        orto_feat = [capital_count, exclamation_count, word_len, char_len]\n",
    "        all_orto_feat.append(orto_feat)\n",
    "    return all_orto_feat\n",
    "\n",
    "orto_feat = EkstraksiOrtografi(raw_tweet)\n",
    "orto_feat[:3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jenis Fitur :  Unigram\n",
      "Akurasi : 0.8573103896320233\n",
      "F1-Score : 0.8572418531351632\n",
      "---------------\n",
      "Jenis Fitur :  Sentimen\n",
      "Akurasi : 0.792892838700005\n",
      "F1-Score : 0.7916838705373135\n",
      "---------------\n",
      "Jenis Fitur :  POS\n",
      "Akurasi : 0.4939323016441506\n",
      "F1-Score : 0.4930192614836379\n",
      "---------------\n",
      "Jenis Fitur :  Ortografi\n",
      "Akurasi : 0.5213864503339941\n",
      "F1-Score : 0.49134936369412757\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# Klasifikasi\n",
    "\n",
    "feat_list = [unigram_feat, sentlex_feat, postag_feat, orto_feat]\n",
    "feat_name = [\"Unigram\", \"Sentimen\", \"POS\", \"Ortografi\"]\n",
    "for f, n in zip(feat_list, feat_name):\n",
    "    X = f\n",
    "    y = label\n",
    "    scoring = ['accuracy', 'f1_macro']\n",
    "    nb = MultinomialNB()\n",
    "    scores = cross_validate(nb, X, y, cv=10, scoring=scoring)\n",
    "    acc = np.mean(scores['test_accuracy'])\n",
    "    f1 = np.mean(scores['test_f1_macro'])\n",
    "    print(\"Jenis Fitur : \", n)\n",
    "    print(\"Akurasi :\", acc)\n",
    "    print(\"F1-Score :\", f1)\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 1], [5, 15], [27, 10], [5, 42], [7, 8], [13, 20], [11, 13], [19, 14], [32, 19], [5, 21]]\n"
     ]
    }
   ],
   "source": [
    "# Leksikon Koto\n",
    "\n",
    "def EkstraksiSentimenKoto(list_tweet):\n",
    "    pos = pd.read_csv(\"dataset/positive_koto.tsv\", delimiter='\\t', index_col=False, header=None, names=['pos', 'sentimen'])\n",
    "    list_pos = pos['pos'].tolist()\n",
    "    list_sent_pos = pos['sentimen'].tolist()\n",
    "    dicti_pos = dict(zip(list_pos, list_sent_pos))\n",
    "    \n",
    "    neg = pd.read_csv(\"dataset/negative_koto.tsv\", delimiter='\\t', index_col=False, header=None, names=['neg', 'sentimen'])\n",
    "    list_neg = neg['neg'].tolist()\n",
    "    list_sent_neg = neg['sentimen'].tolist()\n",
    "    dicti_neg = dict(zip(list_neg, list_sent_neg))\n",
    "    \n",
    "    fitur_sentimen_all = []\n",
    "    for tweet in list_tweet:\n",
    "        ##inisiasi value\n",
    "        emosi = [\"positif\", \"negatif\"]\n",
    "        value = [0,0]\n",
    "        emosi_value = {}\n",
    "        for i in range(len(emosi)):\n",
    "            emosi_value[emosi[i]] = value[i]\n",
    "        list_kata = tweet.split()\n",
    "        for k in list_kata:\n",
    "            if k in dicti_pos.keys():\n",
    "                emosi_value[\"positif\"] += dicti_pos[k]\n",
    "            if k in dicti_neg.keys():\n",
    "                emosi_value[\"negatif\"] += (-1 * dicti_neg[k])\n",
    "        \n",
    "        \n",
    "        fitur_sentimen_perkalimat = list(emosi_value.values())\n",
    "        fitur_sentimen_all.append(fitur_sentimen_perkalimat)\n",
    "        \n",
    "    return fitur_sentimen_all\n",
    "\n",
    "sentlex_koto_feat = EkstraksiSentimenKoto(clean_tweet)\n",
    "print(sentlex_koto_feat[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jenis Model :  Logistic Regression\n",
      "Jenis Fitur :  Unigram\n",
      "Akurasi : 0.8457455314754043\n",
      "F1-Score : 0.8456257675854106\n",
      "---------------\n",
      "Jenis Model :  Decision Tree\n",
      "Jenis Fitur :  Unigram\n",
      "Akurasi : 0.7634472189368825\n",
      "F1-Score : 0.7633470199669499\n",
      "---------------\n",
      "Jenis Model :  Multinomial Naive Bayes\n",
      "Jenis Fitur :  Unigram\n",
      "Akurasi : 0.8472022788226082\n",
      "F1-Score : 0.8471433455046606\n",
      "---------------\n",
      "Jenis Model :  Logistic Regression\n",
      "Jenis Fitur :  Sentimen\n",
      "Akurasi : 0.792892838700005\n",
      "F1-Score : 0.7916838705373135\n",
      "---------------\n",
      "Jenis Model :  Decision Tree\n",
      "Jenis Fitur :  Sentimen\n",
      "Akurasi : 0.7900034981926005\n",
      "F1-Score : 0.7890882123922349\n",
      "---------------\n",
      "Jenis Model :  Multinomial Naive Bayes\n",
      "Jenis Fitur :  Sentimen\n",
      "Akurasi : 0.792892838700005\n",
      "F1-Score : 0.7916838705373135\n",
      "---------------\n",
      "Jenis Model :  Logistic Regression\n",
      "Jenis Fitur :  POS\n",
      "Akurasi : 0.5242582998783962\n",
      "F1-Score : 0.5157061608164644\n",
      "---------------\n",
      "Jenis Model :  Decision Tree\n",
      "Jenis Fitur :  POS\n",
      "Akurasi : 0.5216521463910313\n",
      "F1-Score : 0.516814385938328\n",
      "---------------\n",
      "Jenis Model :  Multinomial Naive Bayes\n",
      "Jenis Fitur :  POS\n",
      "Akurasi : 0.4939323016441506\n",
      "F1-Score : 0.4930192614836379\n",
      "---------------\n",
      "Jenis Model :  Logistic Regression\n",
      "Jenis Fitur :  Ortografi\n",
      "Akurasi : 0.5260015658576401\n",
      "F1-Score : 0.5251079027426016\n",
      "---------------\n",
      "Jenis Model :  Decision Tree\n",
      "Jenis Fitur :  Ortografi\n",
      "Akurasi : 0.5271593010278024\n",
      "F1-Score : 0.5260705429811656\n",
      "---------------\n",
      "Jenis Model :  Multinomial Naive Bayes\n",
      "Jenis Fitur :  Ortografi\n",
      "Akurasi : 0.5213864503339941\n",
      "F1-Score : 0.49134936369412757\n",
      "---------------\n",
      "Jenis Model :  Logistic Regression\n",
      "Jenis Fitur :  Sentimen Koto\n",
      "Akurasi : 0.6897594576135663\n",
      "F1-Score : 0.6892313487184738\n",
      "---------------\n",
      "Jenis Model :  Decision Tree\n",
      "Jenis Fitur :  Sentimen Koto\n",
      "Akurasi : 0.6421190718128967\n",
      "F1-Score : 0.6406677666513143\n",
      "---------------\n",
      "Jenis Model :  Multinomial Naive Bayes\n",
      "Jenis Fitur :  Sentimen Koto\n",
      "Akurasi : 0.6923681098099316\n",
      "F1-Score : 0.6921477402654412\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# Klasifikasi Logistic Regression dan Decision Tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "models = {}\n",
    "models['Logistic Regression'] = LogisticRegression()\n",
    "models['Decision Tree'] = DecisionTreeClassifier()\n",
    "models['Multinomial Naive Bayes'] = MultinomialNB()\n",
    "trainedModels = {}\n",
    "\n",
    "feat_list = [unigram_feat, sentlex_feat, postag_feat, orto_feat, sentlex_koto_feat]\n",
    "feat_name = [\"Unigram\", \"Sentimen\", \"POS\", \"Ortografi\", \"Sentimen Koto\"]\n",
    "zip_feat = zip(feat_list, feat_name)\n",
    "for f, n in zip_feat:\n",
    "    X = f\n",
    "    y = label\n",
    "    scoring = ['accuracy', 'f1_macro']\n",
    "    for i in models.keys():\n",
    "        model = models[i]\n",
    "        scores = cross_validate(model, X, y, cv=10, scoring=scoring)\n",
    "        acc = np.mean(scores['test_accuracy'])\n",
    "        f1 = np.mean(scores['test_f1_macro'])\n",
    "        print(\"Jenis Model : \", i)\n",
    "        print(\"Jenis Fitur : \", n)\n",
    "        print(\"Akurasi :\", acc)\n",
    "        print(\"F1-Score :\", f1)\n",
    "        print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Klasifikasi Logistic Regression dan Decision Tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "test_data = pd.read_csv('dataset/test_set.csv', encoding = \"Latin-1\")\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logResModel = LogisticRegression()\n",
    "logResModel.fit(unigram_feat, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_tweet = pra_pemrosesan(test_data['tweet'])\n",
    "unigram_feat_test = unigram_used.transform(clean_test_tweet)\n",
    "unigram_res = logResModel.predict(unigram_feat_test)\n",
    "df_unigram = test_data[['test_ID']].copy()\n",
    "df_unigram['pred'] = unigram_res\n",
    "df_unigram.to_csv('first_test_pred_unixx1.csv', sep=',', encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNBUni = MultinomialNB()\n",
    "MNBUni.fit(unigram_feat, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_ID</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7970</th>\n",
       "      <td>7970</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7971</th>\n",
       "      <td>7971</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7972</th>\n",
       "      <td>7972</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7973</th>\n",
       "      <td>7973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7974</th>\n",
       "      <td>7974</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7975</th>\n",
       "      <td>7975</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7976</th>\n",
       "      <td>7976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7977</th>\n",
       "      <td>7977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7978</th>\n",
       "      <td>7978</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7979</th>\n",
       "      <td>7979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7980</th>\n",
       "      <td>7980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7981</th>\n",
       "      <td>7981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7982</th>\n",
       "      <td>7982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7983</th>\n",
       "      <td>7983</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7984</th>\n",
       "      <td>7984</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7985</th>\n",
       "      <td>7985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7986</th>\n",
       "      <td>7986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7987</th>\n",
       "      <td>7987</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7988</th>\n",
       "      <td>7988</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7989</th>\n",
       "      <td>7989</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7990</th>\n",
       "      <td>7990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7991</th>\n",
       "      <td>7991</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>7992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7993</th>\n",
       "      <td>7993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>7994</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>7995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>7996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>7997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>7998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>7999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      test_ID  pred\n",
       "0           0     1\n",
       "1           1     0\n",
       "2           2     1\n",
       "3           3     1\n",
       "4           4     1\n",
       "5           5     1\n",
       "6           6     1\n",
       "7           7     1\n",
       "8           8     0\n",
       "9           9     1\n",
       "10         10     1\n",
       "11         11     0\n",
       "12         12     1\n",
       "13         13     1\n",
       "14         14     1\n",
       "15         15     0\n",
       "16         16     0\n",
       "17         17     0\n",
       "18         18     0\n",
       "19         19     1\n",
       "20         20     1\n",
       "21         21     0\n",
       "22         22     1\n",
       "23         23     0\n",
       "24         24     0\n",
       "25         25     0\n",
       "26         26     1\n",
       "27         27     1\n",
       "28         28     0\n",
       "29         29     1\n",
       "...       ...   ...\n",
       "7970     7970     0\n",
       "7971     7971     1\n",
       "7972     7972     0\n",
       "7973     7973     1\n",
       "7974     7974     1\n",
       "7975     7975     1\n",
       "7976     7976     0\n",
       "7977     7977     0\n",
       "7978     7978     0\n",
       "7979     7979     1\n",
       "7980     7980     1\n",
       "7981     7981     0\n",
       "7982     7982     1\n",
       "7983     7983     1\n",
       "7984     7984     0\n",
       "7985     7985     1\n",
       "7986     7986     0\n",
       "7987     7987     1\n",
       "7988     7988     0\n",
       "7989     7989     0\n",
       "7990     7990     0\n",
       "7991     7991     0\n",
       "7992     7992     0\n",
       "7993     7993     1\n",
       "7994     7994     1\n",
       "7995     7995     1\n",
       "7996     7996     1\n",
       "7997     7997     0\n",
       "7998     7998     1\n",
       "7999     7999     0\n",
       "\n",
       "[8000 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_res_MNB = MNBUni.predict(unigram_feat_test)\n",
    "df_unigram_MNB = test_data[['test_ID']].copy()\n",
    "df_unigram_MNB['pred'] = unigram_res_MNB\n",
    "df_unigram_MNB.to_csv('first_test_pred_uni3_MNBxx1.csv', sep=',', encoding='utf-8', index=False, header=False)\n",
    "df_unigram_MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logResModelNew = LogisticRegression()\n",
    "logResModelNew.fit(sentlex_feat, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_ID</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7970</th>\n",
       "      <td>7970</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7971</th>\n",
       "      <td>7971</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7972</th>\n",
       "      <td>7972</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7973</th>\n",
       "      <td>7973</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7974</th>\n",
       "      <td>7974</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7975</th>\n",
       "      <td>7975</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7976</th>\n",
       "      <td>7976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7977</th>\n",
       "      <td>7977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7978</th>\n",
       "      <td>7978</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7979</th>\n",
       "      <td>7979</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7980</th>\n",
       "      <td>7980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7981</th>\n",
       "      <td>7981</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7982</th>\n",
       "      <td>7982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7983</th>\n",
       "      <td>7983</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7984</th>\n",
       "      <td>7984</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7985</th>\n",
       "      <td>7985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7986</th>\n",
       "      <td>7986</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7987</th>\n",
       "      <td>7987</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7988</th>\n",
       "      <td>7988</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7989</th>\n",
       "      <td>7989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7990</th>\n",
       "      <td>7990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7991</th>\n",
       "      <td>7991</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>7992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7993</th>\n",
       "      <td>7993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>7994</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>7995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>7996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>7997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>7998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>7999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      test_ID  pred\n",
       "0           0     1\n",
       "1           1     0\n",
       "2           2     1\n",
       "3           3     1\n",
       "4           4     1\n",
       "5           5     1\n",
       "6           6     1\n",
       "7           7     1\n",
       "8           8     1\n",
       "9           9     1\n",
       "10         10     1\n",
       "11         11     0\n",
       "12         12     0\n",
       "13         13     1\n",
       "14         14     1\n",
       "15         15     0\n",
       "16         16     0\n",
       "17         17     1\n",
       "18         18     0\n",
       "19         19     1\n",
       "20         20     0\n",
       "21         21     0\n",
       "22         22     1\n",
       "23         23     0\n",
       "24         24     1\n",
       "25         25     0\n",
       "26         26     1\n",
       "27         27     1\n",
       "28         28     1\n",
       "29         29     1\n",
       "...       ...   ...\n",
       "7970     7970     0\n",
       "7971     7971     0\n",
       "7972     7972     1\n",
       "7973     7973     0\n",
       "7974     7974     1\n",
       "7975     7975     0\n",
       "7976     7976     0\n",
       "7977     7977     0\n",
       "7978     7978     1\n",
       "7979     7979     0\n",
       "7980     7980     1\n",
       "7981     7981     1\n",
       "7982     7982     1\n",
       "7983     7983     1\n",
       "7984     7984     0\n",
       "7985     7985     1\n",
       "7986     7986     1\n",
       "7987     7987     1\n",
       "7988     7988     0\n",
       "7989     7989     1\n",
       "7990     7990     0\n",
       "7991     7991     0\n",
       "7992     7992     1\n",
       "7993     7993     1\n",
       "7994     7994     1\n",
       "7995     7995     1\n",
       "7996     7996     1\n",
       "7997     7997     0\n",
       "7998     7998     1\n",
       "7999     7999     0\n",
       "\n",
       "[8000 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentlex_feat_test = EkstraksiSentimen(clean_test_tweet)\n",
    "sentiment_res = logResModelNew.predict(sentlex_feat_test)\n",
    "df_sentiment = test_data[['test_ID']].copy()\n",
    "df_sentiment['pred'] = sentiment_res\n",
    "df_sentiment.to_csv('first_test_pred_sentimentxx1.csv', sep=',', encoding='utf-8', index=False, header=False)\n",
    "df_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
