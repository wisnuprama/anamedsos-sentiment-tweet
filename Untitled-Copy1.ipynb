{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3462, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentimen</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>oks kak semangat ya kalian kalian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>sekarang harus kaya orang bodoh lagi bodoh sangat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>begitu diumumkan lulus 100 mereka semua sujud ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>katanya bapak reformasi dan demokrasi di neger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>macet macetan perut kosong akhirnya mampir dah...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  sentimen                                              tweet\n",
       "0   1         1                  oks kak semangat ya kalian kalian\n",
       "1   2         0  sekarang harus kaya orang bodoh lagi bodoh sangat\n",
       "2   3         1  begitu diumumkan lulus 100 mereka semua sujud ...\n",
       "3   4         0  katanya bapak reformasi dan demokrasi di neger...\n",
       "4   5         0  macet macetan perut kosong akhirnya mampir dah..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Membaca data menggunakan pandas\n",
    "#import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import Sastrawi\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import svm\n",
    "from nltk.tag import CRFTagger\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "raw_data = pd.read_csv('git/anamedsos-sentiment-tweet/dataset/preprocess_state.csv', encoding = \"Latin-1\")\n",
    "print(raw_data.shape)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi Normalisasi\n",
    "\n",
    "def normalisasi(tweet):\n",
    "    normal_tw = tweet.lower() #lowercase\n",
    "    normal_tw = re.sub('\\s+', ' ', normal_tw) # remove extra space\n",
    "    normal_tw = normal_tw.strip() #trim depan belakang\n",
    "    normal_tw = re.sub(r'[^\\w\\s]','',normal_tw) #buang punctuation\n",
    "    normal_regex = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE) #regex huruf yang berulang kaya haiiii (untuk fitur unigram)\n",
    "    normal_tw = normal_regex.sub(r\"\\1\\1\", normal_tw) #buang huruf yang berulang\n",
    "    return normal_tw\n",
    "\n",
    "# Fungsi Remove Stopwords\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    stopwords = pd.read_csv(\"dataset/stopwords.csv\")\n",
    "    special_list = ['username', 'url', 'sensitive-no']\n",
    "    token = nltk.word_tokenize(tweet)\n",
    "    token_afterremoval = []\n",
    "    for k in token:\n",
    "        if k not in stopwords and k not in special_list:\n",
    "            token_afterremoval.append(k)\n",
    "    \n",
    "    str_clean = ' '.join(token_afterremoval)\n",
    "    return str_clean\n",
    "\n",
    "# Fungsi Stemming\n",
    "\n",
    "def Stemming(tweet):\n",
    "    token = nltk.word_tokenize(tweet)\n",
    "    stem_kalimat = []\n",
    "    for k in token:\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "        stem_kata = stemmer.stem(clean_tw)\n",
    "        stem_kalimat.append(stem_kata)\n",
    "        \n",
    "    stem_kalimat_str = ' '.join(stem_kalimat)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oks kak semangat ya kalian kalian',\n",
       " 'sekarang harus kaya orang bodoh lagi bodoh sangat',\n",
       " 'begitu diumumkan lulus num mereka semua sujud syukur dan langsung mengambil bungasaat dia menghampiri langsung memeluk menciumku air mata tak kuasa kubendungmom this is my birthday present for u']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prapemrosesan Data\n",
    "\n",
    "def pra_pemrosesan(list_tweet):\n",
    "    tweet_clean = []\n",
    "    for tw in list_tweet:\n",
    "        normal_tweet = normalisasi(tw)\n",
    "        nosw_tweet = remove_stopwords(normal_tweet)\n",
    "        #stem_tweet = Stemming(nosw_tweet)\n",
    "        tweet_clean.append(nosw_tweet)\n",
    "    return tweet_clean\n",
    "\n",
    "raw_tweet = raw_data['tweet'].replace('\\d+', 'num', regex=True)\n",
    "label = raw_data['sentimen'].tolist()\n",
    "\n",
    "clean_tweet = pra_pemrosesan(raw_tweet)\n",
    "clean_tweet[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "['__', '_ããâ½', 'aa', 'aah', 'aamiin', 'abadi', 'abal', 'abang', 'abis', 'about']\n"
     ]
    }
   ],
   "source": [
    "# Ekstraksi Bag Of Word 2000 fitur\n",
    "\n",
    "def EkstraksiBoW2000(tweet):\n",
    "    unigram = CountVectorizer(ngram_range=(1,1), max_features=5000)\n",
    "    unigram_matrix = unigram.fit_transform(np.array(tweet)).todense()\n",
    "    nama_fitur = unigram.get_feature_names()\n",
    "    return unigram_matrix, nama_fitur, unigram\n",
    "\n",
    "unigram_feat_2000, feat_name_2000, unigram_used_2000 = EkstraksiBoW2000(clean_tweet)\n",
    "print(unigram_feat_2000[:3])\n",
    "print(feat_name_2000[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "['__', '_gue', '_kngdanielnum', '_lo', '_rt', '_ã', '_ãâ½', '_ããâ½', '_ããâ½ãâ½', 'aa']\n"
     ]
    }
   ],
   "source": [
    "# Ekstraksi Bag Of Word\n",
    "\n",
    "def EkstraksiBoW(tweet):\n",
    "    unigram = CountVectorizer(ngram_range=(1,1))\n",
    "    unigram_matrix = unigram.fit_transform(np.array(tweet)).todense()\n",
    "    nama_fitur = unigram.get_feature_names()\n",
    "    return unigram_matrix, nama_fitur, unigram\n",
    "\n",
    "unigram_feat, feat_name, unigram_used = EkstraksiBoW(clean_tweet)\n",
    "print(unigram_feat[:3])\n",
    "print(feat_name[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0], [0, 3], [1, 0], [2, 0], [0, 2], [0, 2], [0, 0], [2, 0], [1, 0], [0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Leksikon\n",
    "\n",
    "def EkstraksiSentimen(list_tweet):\n",
    "    pos = pd.read_csv(\"dataset/positif_vania.txt\", header=None, names=['pos'])\n",
    "    list_pos = pos['pos'].tolist()\n",
    "    neg = pd.read_csv(\"dataset/negatif_vania.txt\", header=None, names=['neg'])\n",
    "    list_neg = neg['neg'].tolist()\n",
    "    \n",
    "    fitur_sentimen_all = []\n",
    "    for tweet in list_tweet:\n",
    "        ##inisiasi value\n",
    "        emosi = [\"positif\", \"negatif\"]\n",
    "        value = [0,0]\n",
    "        emosi_value = {}\n",
    "        for i in range(len(emosi)):\n",
    "            emosi_value[emosi[i]] = value[i]\n",
    "        list_kata = tweet.split()\n",
    "        for k in list_kata:\n",
    "            if k in list_pos:\n",
    "                emosi_value[\"positif\"] += 1\n",
    "            if k in list_neg:\n",
    "                emosi_value[\"negatif\"] += 1\n",
    "        \n",
    "        \n",
    "        fitur_sentimen_perkalimat = list(emosi_value.values())\n",
    "        fitur_sentimen_all.append(fitur_sentimen_perkalimat)\n",
    "        \n",
    "    return fitur_sentimen_all\n",
    "\n",
    "sentlex_feat = EkstraksiSentimen(clean_tweet)\n",
    "print(sentlex_feat[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 0), (0, 1)]\n"
     ]
    }
   ],
   "source": [
    "# POSTag\n",
    "\n",
    "def EkstraksiPOS(list_tweet):\n",
    "    ct = CRFTagger()\n",
    "    ct.set_model_file(\"dataset/all_indo_man_tag_corpus_model.crf.tagger\")\n",
    "    pos_feat_list = []\n",
    "    count_tag = []\n",
    "    for tweet in list_tweet: \n",
    "        token = nltk.word_tokenize(tweet)\n",
    "        tag = ct.tag_sents([token])\n",
    "        flat_tag = [item for sublist in tag for item in sublist]\n",
    "        pos_count = Counter([j for i,j in flat_tag])\n",
    "        pos_feat = (pos_count['JJ'], pos_count['NEG'])\n",
    "        pos_feat_list.append(pos_feat)\n",
    "    return pos_feat_list\n",
    "\n",
    "postag_feat = EkstraksiPOS(clean_tweet)\n",
    "print(postag_feat[:3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 6, 33], [0, 0, 8, 49], [0, 0, 29, 194]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ortografi\n",
    "\n",
    "def EkstraksiOrtografi(raw_tweet):\n",
    "    all_orto_feat = []\n",
    "    for tw in raw_tweet:\n",
    "        capital_count = sum(1 for c in tw if c.isupper())\n",
    "        exclamation_count = sum((1 for c in tw if c == \"!\"))\n",
    "        word_len = len(nltk.word_tokenize(tw))\n",
    "        char_len = len(tw)\n",
    "        orto_feat = [capital_count, exclamation_count, word_len, char_len]\n",
    "        all_orto_feat.append(orto_feat)\n",
    "    return all_orto_feat\n",
    "\n",
    "orto_feat = EkstraksiOrtografi(raw_tweet)\n",
    "orto_feat[:3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 1], [5, 15], [27, 10], [5, 42], [7, 8], [13, 20], [11, 13], [19, 14], [32, 19], [5, 21]]\n"
     ]
    }
   ],
   "source": [
    "# Leksikon Koto\n",
    "\n",
    "def EkstraksiSentimenKoto(list_tweet):\n",
    "    pos = pd.read_csv(\"dataset/positive_koto.tsv\", delimiter='\\t', index_col=False, header=None, names=['pos', 'sentimen'])\n",
    "    list_pos = pos['pos'].tolist()\n",
    "    list_sent_pos = pos['sentimen'].tolist()\n",
    "    dicti_pos = dict(zip(list_pos, list_sent_pos))\n",
    "    \n",
    "    neg = pd.read_csv(\"dataset/negative_koto.tsv\", delimiter='\\t', index_col=False, header=None, names=['neg', 'sentimen'])\n",
    "    list_neg = neg['neg'].tolist()\n",
    "    list_sent_neg = neg['sentimen'].tolist()\n",
    "    dicti_neg = dict(zip(list_neg, list_sent_neg))\n",
    "    \n",
    "    fitur_sentimen_all = []\n",
    "    for tweet in list_tweet:\n",
    "        ##inisiasi value\n",
    "        emosi = [\"positif\", \"negatif\"]\n",
    "        value = [0,0]\n",
    "        emosi_value = {}\n",
    "        for i in range(len(emosi)):\n",
    "            emosi_value[emosi[i]] = value[i]\n",
    "        list_kata = tweet.split()\n",
    "        for k in list_kata:\n",
    "            if k in dicti_pos.keys():\n",
    "                emosi_value[\"positif\"] += dicti_pos[k]\n",
    "            if k in dicti_neg.keys():\n",
    "                emosi_value[\"negatif\"] += (-1 * dicti_neg[k])\n",
    "        \n",
    "        \n",
    "        fitur_sentimen_perkalimat = list(emosi_value.values())\n",
    "        fitur_sentimen_all.append(fitur_sentimen_perkalimat)\n",
    "        \n",
    "    return fitur_sentimen_all\n",
    "\n",
    "sentlex_koto_feat = EkstraksiSentimenKoto(clean_tweet)\n",
    "print(sentlex_koto_feat[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_list = [sentlex_feat, sentlex_koto_feat, postag_feat, orto_feat]\n",
    "feat_name = [\"Sentimen\", \"Sentimen Koto\",\"POS\", \"Ortografi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Jenis Model :  Logistic Regression\n",
      "Jenis Fitur :  Sentimen\n",
      "Akurasi : 0.792892838700005\n",
      "F1-Score : 0.7916838705373135\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Decision Tree\n",
      "Jenis Fitur :  Sentimen\n",
      "Akurasi : 0.7900034981926005\n",
      "F1-Score : 0.7890882123922349\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Multinomial Naive Bayes\n",
      "Jenis Fitur :  Sentimen\n",
      "Akurasi : 0.792892838700005\n",
      "F1-Score : 0.7916838705373135\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Random Forest\n",
      "Jenis Fitur :  Sentimen\n",
      "Akurasi : 0.7905823657776814\n",
      "F1-Score : 0.7896126316929742\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  KNN\n",
      "Jenis Fitur :  Sentimen\n",
      "Akurasi : 0.7547400509736637\n",
      "F1-Score : 0.7515928271362384\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Linear Discriminant Analysis\n",
      "Jenis Fitur :  Sentimen\n",
      "Akurasi : 0.792892838700005\n",
      "F1-Score : 0.7916838705373135\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Logistic Regression\n",
      "Jenis Fitur :  Sentimen Koto\n",
      "Akurasi : 0.6897594576135663\n",
      "F1-Score : 0.6892313487184738\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Decision Tree\n",
      "Jenis Fitur :  Sentimen Koto\n",
      "Akurasi : 0.6432734753710583\n",
      "F1-Score : 0.641853671472942\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Multinomial Naive Bayes\n",
      "Jenis Fitur :  Sentimen Koto\n",
      "Akurasi : 0.6923681098099316\n",
      "F1-Score : 0.6921477402654412\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Random Forest\n",
      "Jenis Fitur :  Sentimen Koto\n",
      "Akurasi : 0.6796605087371524\n",
      "F1-Score : 0.6789790682938228\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  KNN\n",
      "Jenis Fitur :  Sentimen Koto\n",
      "Akurasi : 0.6762022954806683\n",
      "F1-Score : 0.6759350357347292\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Linear Discriminant Analysis\n",
      "Jenis Fitur :  Sentimen Koto\n",
      "Akurasi : 0.6894737718845263\n",
      "F1-Score : 0.6887419813925304\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Logistic Regression\n",
      "Jenis Fitur :  POS\n",
      "Akurasi : 0.5225250287351534\n",
      "F1-Score : 0.5141965089868015\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Decision Tree\n",
      "Jenis Fitur :  POS\n",
      "Akurasi : 0.5181864370075461\n",
      "F1-Score : 0.5090863802049637\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Multinomial Naive Bayes\n",
      "Jenis Fitur :  POS\n",
      "Akurasi : 0.49479768786127165\n",
      "F1-Score : 0.4937952376417506\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Random Forest\n",
      "Jenis Fitur :  POS\n",
      "Akurasi : 0.524249970848395\n",
      "F1-Score : 0.5110098751682715\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  KNN\n",
      "Jenis Fitur :  POS\n",
      "Akurasi : 0.49478019689826913\n",
      "F1-Score : 0.47208185832973204\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Linear Discriminant Analysis\n",
      "Jenis Fitur :  POS\n",
      "Akurasi : 0.5225250287351534\n",
      "F1-Score : 0.5141965089868015\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Logistic Regression\n",
      "Jenis Fitur :  Ortografi\n",
      "Akurasi : 0.5514067731671969\n",
      "F1-Score : 0.5477403118668196\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Decision Tree\n",
      "Jenis Fitur :  Ortografi\n",
      "Akurasi : 0.5493894821009145\n",
      "F1-Score : 0.5462470194652937\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Multinomial Naive Bayes\n",
      "Jenis Fitur :  Ortografi\n",
      "Akurasi : 0.5522713264813179\n",
      "F1-Score : 0.5366296279728153\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Random Forest\n",
      "Jenis Fitur :  Ortografi\n",
      "Akurasi : 0.5361196715030567\n",
      "F1-Score : 0.5342923098027748\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  KNN\n",
      "Jenis Fitur :  Ortografi\n",
      "Akurasi : 0.5517041195382386\n",
      "F1-Score : 0.5509289278078\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Jenis Model :  Linear Discriminant Analysis\n",
      "Jenis Fitur :  Ortografi\n",
      "Akurasi : 0.5496735020239543\n",
      "F1-Score : 0.5458033676208885\n",
      "--------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Klasifikasi Logistic Regression dan Decision Tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "models = {}\n",
    "models['Logistic Regression'] = LogisticRegression()\n",
    "models['Decision Tree'] = DecisionTreeClassifier()\n",
    "models['Multinomial Naive Bayes'] = MultinomialNB()\n",
    "models['Random Forest'] = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "models['KNN'] = KNeighborsClassifier(n_neighbors=7)\n",
    "models['Linear Discriminant Analysis'] = LinearDiscriminantAnalysis()\n",
    "models['SVM'] = svm.SVC(gamma='scale')\n",
    "\n",
    "zip_feat = zip(feat_list, feat_name)\n",
    "\n",
    "for f, n in zip_feat:\n",
    "    X = f\n",
    "    y = label\n",
    "    scoring = ['accuracy', 'f1_macro']\n",
    "    for i in models.keys():\n",
    "        try:\n",
    "            model = models[i]\n",
    "            scores = cross_validate(model, X, y, cv=10, scoring=scoring)\n",
    "            acc = np.mean(scores['test_accuracy'])\n",
    "            f1 = np.mean(scores['test_f1_macro'])\n",
    "            print(\"--------------------------------\")\n",
    "            print(\"Jenis Model : \", i)\n",
    "            print(\"Jenis Fitur : \", n)\n",
    "            print(\"Akurasi :\", acc)\n",
    "            print(\"F1-Score :\", f1)\n",
    "            print(\"--------------------------------\")\n",
    "            print(\"\\n\")\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_ID</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Jadi wanita jangan suka menghancurkan hubungan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sombong apanya kalau sms saja dibls terus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>apadah kamu :p cie cie baik kamu cie bebe cie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tdrlah besok medical check up semoga lancar â?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>crew serbu bsm seru (at bank syariah mandiri b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_ID                                              tweet\n",
       "0        0  Jadi wanita jangan suka menghancurkan hubungan...\n",
       "1        1          sombong apanya kalau sms saja dibls terus\n",
       "2        2  apadah kamu :p cie cie baik kamu cie bebe cie ...\n",
       "3        3  tdrlah besok medical check up semoga lancar â?...\n",
       "4        4  crew serbu bsm seru (at bank syariah mandiri b..."
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('dataset/test_set.csv', encoding = \"Latin-1\")\n",
    "print(test_data.shape)\n",
    "clean_test_tweet = pra_pemrosesan(test_data['tweet'])\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jadi wanita jangan suka menghancurkan hubungan orang jgn bangga berhasil merusak kebahagian orang silahkan saja tapi ga berkah bahagianya nanti hehe',\n",
       " 'sombong apanya kalau sms saja dibls terus',\n",
       " 'apadah kamu p cie cie baik kamu cie bebe cie kiwkiw']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_tweet[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_feat_test_2000 = unigram_used_2000.transform(clean_test_tweet)\n",
    "unigram_feat_test = unigram_used.transform(clean_test_tweet)\n",
    "sentlex_feat_test = EkstraksiSentimen(clean_test_tweet)\n",
    "postag_feat_test = EkstraksiPOS(clean_test_tweet)\n",
    "orto_feat_test = EkstraksiOrtografi(clean_test_tweet)\n",
    "sentlex_koto_feat_test = EkstraksiSentimenKoto(clean_test_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logResModel = LogisticRegression()\n",
    "logResModel.fit(unigram_feat_2000, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_res = logResModel.predict(unigram_feat_test_2000)\n",
    "df_unigram = test_data[['test_ID']].copy()\n",
    "df_unigram['pred'] = unigram_res\n",
    "df_unigram.to_csv('first_test_pred_uni6_1.csv', sep=',', encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "logResModel2 = LogisticRegression()\n",
    "logResModel2.fit(unigram_feat, label)\n",
    "unigram_res2 = logResModel2.predict(unigram_feat_test)\n",
    "df_unigram2 = test_data[['test_ID']].copy()\n",
    "df_unigram2['pred'] = unigram_res2\n",
    "df_unigram2.to_csv('first_test_pred_uni6_2.csv', sep=',', encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNBUni = MultinomialNB()\n",
    "MNBUni.fit(unigram_feat, label)\n",
    "unigram_res_MNB = MNBUni.predict(unigram_feat_test)\n",
    "df_unigram_MNB = test_data[['test_ID']].copy()\n",
    "df_unigram_MNB['pred'] = unigram_res_MNB\n",
    "df_unigram_MNB.to_csv('first_test_pred_uni3_MNB2.csv', sep=',', encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNBUni2 = MultinomialNB()\n",
    "MNBUni2.fit(unigram_feat_2000, label)\n",
    "unigram_res_MNB2 = MNBUni2.predict(unigram_feat_test_2000)\n",
    "df_unigram_MNB = test_data[['test_ID']].copy()\n",
    "df_unigram_MNB['pred'] = unigram_res_MNB2\n",
    "df_unigram_MNB.to_csv('first_test_pred_uni3_MNB3.csv', sep=',', encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
